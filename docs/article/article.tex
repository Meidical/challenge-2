\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{microtype}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage[hidelinks]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Donor-Recipient Matching and Allocation
    in Pediatric Bone-Marrow Transplantation\\}

\author{
    \fontsize{10pt}{11pt}\selectfont
    \begin{tabular}{ccc}
        \begin{tabular}{c}
            Danilo Silva                                   \\
            \textit{Department of Artificial Intelligence} \\
            \textit{Polytechnic School of Porto}           \\
            Porto, Portugal                                \\
            1250424@isep.ipp.pt
        \end{tabular}
         &
        \begin{tabular}{c}
            Luís Magalhães                                 \\
            \textit{Department of Artificial Intelligence} \\
            \textit{Polytechnic School of Porto}           \\
            Porto, Portugal                                \\
            1100628@isep.ipp.pt
        \end{tabular}
         &
        \begin{tabular}{c}
            José Domingues                                 \\
            \textit{Department of Artificial Intelligence} \\
            \textit{Polytechnic School of Porto}           \\
            Porto, Portugal                                \\
            1000984@isep.ipp.pt
        \end{tabular}
        \\[6ex]
        \begin{tabular}{c}
            Ricardo Sousa                                  \\
            \textit{Department of Artificial Intelligence} \\
            \textit{Polytechnic School of Porto}           \\
            Porto, Portugal                                \\
            1201856@isep.ipp.pt
        \end{tabular}
         &
        \begin{tabular}{c}
            Tomás Pereira                                  \\
            \textit{Department of Artificial Intelligence} \\
            \textit{Polytechnic School of Porto}           \\
            Porto, Portugal                                \\
            1210830@isep.ipp.pt
        \end{tabular}
    \end{tabular}
}

\maketitle

\begin{abstract}
Pediatric allogeneic hematopoietic stem cell transplantation (HSCT) is a potentially curative therapy for malignant and non-malignant diseases, but outcomes depend strongly on donor-recipient compatibility and graft characteristics. Donor selection is therefore a high-stakes, time-sensitive decision in which clinicians must balance immunogenetic risk against urgency and donor availability.
This work proposes a decision-support system integrated with predictive artificial intelligence models. The multi-criteria decision-making method TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) is employed to rank and select the most suitable donor for a given recipient. One of the decision criteria is the predicted post-transplant survival time of the recipient (benefit criterion), which is estimated using a machine learning model trained on a published pediatric unrelated-donor cohort.
The overall goal is to support transparent, reproducible, and clinically meaningful donor-recipient matching and allocation decisions in pediatric HSCT.
Experimental results demonstrate that an XGBoost (eXtreme Gradient Boosting) model with the help of Generative Adversarial Networks (GANs) achieved the best predictive performance for post-transplant probability of survival, reaching an F1-Score of 0.65. This probability was then used to predict the expected post-transplant survival time in case of death. For this regression task, the best performing model was an Extra Trees Regressor model, which achieved a Root Mean Square Error (RMSE) of 560, significantly outperforming baseline level (913 RMSE).

\end{abstract}


\begin{IEEEkeywords}
    Pediatric hematopoietic stem cell transplantation,
    donor-recipient matching, machine learning, data-driven decision support, multi-criteria decision making
\end{IEEEkeywords}


\section{Introduction}
Allogeneic hematopoietic stem cell transplantation (HSCT) is an established therapeutic option for a wide range of malignant and non-malignant hematologic conditions in pediatric populations and often represents the standard of care in high-risk cases; however, it carries substantial clinical risks, such as graft-versus-host disease and transplant-related mortality, which necessitate careful donor selection~\cite{Diaz2024}. 
In clinical practice, transplant success is tightly linked to donor-recipient compatibility and to graft characteristics (the material transplanted from the donor to the recipient, such as CD34$^{+}$ and CD3$^{+}$ cells), because immunologic disparity can increase complications such as graft-versus-host disease (GVHD), graft failure, and transplant-related mortality \cite{Tiercy2016}. 
Donor selection is typically guided by high-resolution Human Leukocyte Antigen (HLA) matching and additional donor/recipient factors (e.g., Cytomegalovirus (CMV) status, age, stem cell source - bone marrow or peripheral blood stem cells), while acknowledging that not all patients have access to an ideal matched sibling and that alternative donor options are frequently required \cite{Tiercy2016}. 
Beyond “match counts,” functional and locus-specific approaches, referring to specific genomic positions associated with immunogenetic variability, can refine immunological risk; for example, classification of HLA-DPB1 mismatches into permissive vs non-permissive groups has been associated with clinically relevant differences in outcomes \cite{Fleischhauer2012}. In parallel, the availability of large transplantation registries and richer clinical data has motivated machine learning (ML) approaches to outcome prediction (e.g., early mortality, GVHD risk). Prior work has shown that ML can produce clinically meaningful risk stratification, but performance and generalizability depend heavily on data preparation choices and robust validation designs \cite{Shouval2015}, \cite{Arai2019}, \cite{Tang2020}. \\

Despite established donor selection principles, there is still a lack of integrated, transparent frameworks that (i) formalize donor-recipient allocation under multiple clinical criteria and (ii) incorporate data-driven outcome predictions using only pre-decision variables. This work proposes a decision-support system integrated with predictive artificial intelligence models. Donor allocation is formulated as a multi-criteria decision-making problem, in which alternative ranking is performed using the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS), with criterion weights calculated by the Analytic Hierarchy Process (AHP). Recipient post-transplant survival time is estimated by a machine learning model trained on a dataset \cite{Sikora2010} derived from a published pediatric unrelated-donor cohort \cite{Kalwak2010}. The predicted survival outcome is incorporated as one of the decision criteria, yielding an ordered list of candidate matches for clinical review.

\section{State-of-the-Art}

Allogeneic hematopoietic stem cell transplantation (HSCT) stands as the unique curative option for various pediatric patients with malignant and non-malignant hematologic diseases \cite{Diaz2024}. The success of the transplant critically depends on Human Leukocyte Antigen (HLA) compatibility between the donor and recipient. Although the gold standard is a 10/10 genotypic match (HLA-A, -B, -C, -DRB1, -DQB1), approximately 70\% of patients do not have a compatible family donor, necessitating the use of unrelated or haploidentical donors \cite{Shouval2015}.

The complexity of immunogenetics, with thousands of known HLA alleles, combined with the heterogeneity of clinical data, has rendered traditional statistical approaches (such as Cox regression) insufficient for capturing complex non-linear interactions. Consequently, the application of Machine Learning (ML) algorithms has emerged as a vital tool for predicting outcomes (survival, Graft-versus-Host Disease (GVHD)) and supporting donor allocation \cite{Shouval2014}, \cite{Gupta2020}.

This section reviews the literature, focusing on data preparation and model validation methodologies.

\subsection{Data preparation and feature engineering}

The quality of ML models depends intrinsically on data preparation. The literature identifies three critical vectors in this phase:\\

\begin{enumerate}
    \item \textbf{HLA resolution and complexity: } high-resolution typing (allelic level) is fundamental. Lee et al. \cite{Lee2007} demonstrated in a landmark study that high-resolution matching at HLA-A, -B, -C, and -DRB1 is associated with higher survival rates. Simple binary categorization (matched/mismatched) is insufficient; recent models incorporate the distinction between ``permissive'' and ``non-permissive'' mismatches, particularly at the HLA-DPB1 locus, based on T-cell epitopes. Fleischhauer et al. \cite{Fleischhauer2012} validated that non-permissive mismatches significantly increase mortality, making this a crucial feature for predictive algorithms. \\

    \item \textbf{Specific clinical and pediatric variables: } beyond HLA, feature engineering must include donor and graft-specific factors. Kałwak et al. \cite{Kalwak2010}, in a study focused on pediatrics, highlighted that higher doses of CD34\textsuperscript{+} and CD3\textsuperscript{+} cells in the graft promote better long-term survival without increasing the risk of severe GVHD. The inclusion of these quantitative biological variables enriches ML models. Additionally, Tang et al. \cite{Tang2020} innovated by using longitudinal vital sign data (e.g., temperature, blood pressure) extracted from Electronic Health Records (EHR), demonstrating that temporal trends (slopes) are stronger predictors of acute GVHD than static measurements.\\

    \item \textbf{Missing data treatment and feature selection: } because real-world databases may contain noise and missing data, Shouval et al. \cite{Shouval2014} discuss the need for robust preprocessing, including imputation and discretization. Feature selection is critical to avoid hyper-dimensionality. For instance, in a data mining study involving 28,236 patients, the Alternating Decision Tree (ADTree) algorithm automatically selected 10 out of 20 possible variables, eliminating redundancies (e.g., combining donor/recipient Cytomegalovirus (CMV) serostatus into a single interaction variable) \cite{Shouval2015}.\\
\end{enumerate}


\subsection{ML models and validation strategies}

The transition from classical statistical models to ML requires rigorous validation to avoid overfitting, where the model memorizes training data but fails to generalize \cite{Shouval2014}.

\begin{enumerate}
    \item \textbf{Predictive algorithms:} recent literature favors algorithms that balance accuracy with clinical interpretability:

          \begin{itemize}
              \item \textbf{Decision trees and ensemble methods:} Shouval et al. \cite{Shouval2015} and Arai et al. \cite{Arai2019} successfully used the ADTree algorithm to predict mortality and GVHD, respectively. ADTree was preferred over Artificial Neural Networks (ANN) or Random Forests because it allows for the visualization of decision rules and interactions (e.g., the impact of disease stage varies by age), whereas ``black box'' models hide this logic.

              \item \textbf{Penalized logistic regression:} Tang et al. \cite{Tang2020} used L2 regularization to handle collinearity in longitudinal vital sign data, outperforming baseline models that used only static characteristics.\\
          \end{itemize}

    \item \textbf{Validation methodologies:} robust validation is consistent across high-quality studies:

          \begin{itemize}
              \item \textbf{Train/test split:} Arai et al.  \cite{Arai2019}. randomly divided a cohort of 26,695 patients into training (70\%) and validation (30\%) sets. The trained model was tested on the validation cohort, demonstrating clear risk stratification (hazard ratio 2.57 for high risk vs. low risk).

              \item \textbf{Cross-validation:} Both Shouval et al. \cite{Shouval2014} and Gupta et al. \cite{Gupta2020} advocate for the use of 10-fold cross-validation on the training set to optimize hyperparameters prior to final testing.

              \item \textbf{Calibration:} accuracy (AUC) is not the only metric. Shouval et al. \cite{Shouval2014} emphasize the importance of calibration (agreement between predicted and observed probability), demonstrating excellent consistency in their 100-day mortality model .\\
          \end{itemize}
\end{enumerate}

\subsection{From prediction to allocation}
While ML models provide a risk score (prediction), clinical decision-making requires selecting the best donor among several available options (allocation). The literature suggests a ``prediction-to-decision'' gap \cite{Shouval2014}. Multiple Attribute Decision Making (MADM) methodologies, such as TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), described by Tzeng \& Huang \cite{Tzeng2011}, allow for the integration of ML predictions (as criteria) into an ordered ranking of alternatives. This hybrid approach (ML to predict outputs, TOPSIS to rank candidates) represents an improvement for clinical decision support systems, transforming raw probabilities into actionable recommendations.

\section{Methodology}
The proposed methodology formulates pediatric bone marrow donor–recipient allocation as a multi-criteria decision-making problem. The objective is to generate a ranked list of feasible donor–recipient pairs and provide transparent decision support to the responsible clinician, who retains full responsibility for the final donor selection.
Alternative ranking is performed using the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). Criterion weights are obtained using the Analytic Hierarchy Process (AHP), which is applied exclusively to compute the preference vector (weights) of the decision criteria and validated through consistency analysis (consistency ratio). AHP is not applied directly to rank alternatives due to scalability limitations when the number of potential donors is large, which would require a quadratic number of pairwise comparisons.
The proposed framework explicitly separates criteria weighting (AHP), predictive modeling (machine learning), and final ranking (TOPSIS), thereby supporting transparency, clinical interpretability, and auditability.
The decision criteria considered in the ranking process are: HLA Match, CMV Serostatus, Donor Age Group, Gender Match, ABO Match, and Expected Survival Time. The relative importance for these criteria was defined through literature review, as shown in Table~\ref{tab:decision_criteria}. The Predicted Survival Time has the lowest value of importance, as it is a supplementary criterion to support decision-making rather than the primary driver of donor selection. Because hematopoietic stem cell can have two sources (bone marrow or peripheral blood), if the source is bone marrow, the ABO compatibility criterion will have an higher importance than donor-recipient sex match, and if the source is peripheral blood, the ABO compatibility criterion will have a lower importance than donor-recipient sex match, because in this process the blood is not manipulated as in bone marrow donation, where red blood cells are removed to avoid hemolysis in case of ABO incompatibility~\cite{jimenez_jimenez_allogeneic_2025}.

\begin{table*}[htbp]
\caption{Decision Criteria for Donor--Recipient Matching}
\label{tab:decision_criteria}
\centering
\begin{tabularx}{\textwidth}{c l c X c}
\hline
\textbf{Rank} & \textbf{Criterion} & \textbf{Priority} & \textbf{Justification} & \textbf{Sources} \\
\hline
1 & HLA Compatibility & Critical & 
Remains the primary determinant of transplant outcome. High-resolution matching at HLA-A, -B, -C and -DRB1 is the gold standard. Mismatches are consistently associated with reduced survival and increased complications, making HLA assessment the starting point of donor selection, even among alternative donors. &
\cite{dehn_selection_2019, kollman_effect_2016,jimenez_jimenez_allogeneic_2025} \\
\hline
2 & Donor Age & Very High & 
The most consistently validated non-HLA factor associated with overall survival. Younger donors (18--32 years) are associated with superior outcomes. Each 10-year increase in donor age increases mortality risk by approximately 5.5\%, and donor age should therefore be prioritized over other secondary factors. &
\cite{kollman_effect_2016,jimenez_jimenez_allogeneic_2025} \\
\hline
3 & CMV Serostatus & High & 
Particularly relevant for CMV-negative recipients, who benefit from CMV-negative donors to reduce non-relapse mortality. However, the overall impact on survival has decreased due to improved antiviral prophylaxis, making this criterion secondary to HLA and donor age. &
\cite{kollman_effect_2016,jimenez_jimenez_allogeneic_2025, ljungman_guidelines_2019} \\
\hline
4 & ABO Compatibility & Moderate & 
The impact on overall survival is modest or not statistically significant in recent cohorts. Its relevance is mainly operational, reducing hemolysis risk and transfusion requirements. It becomes more important when bone marrow is the graft source, where major ABO mismatches may reduce usable cell yield during processing. &
\cite{jimenez_jimenez_allogeneic_2025} \\
\hline
5 & Donor--Recipient Sex Match & Low & 
Female-to-male transplants are associated with increased GVHD risk due to anti-HY alloreactivity and delayed neutrophil recovery. Current guidelines consider this a minor criterion, mainly useful for tie-breaking between otherwise equivalent donors. &
\cite{jimenez_jimenez_allogeneic_2025,gratwohl_gender_2016} \\
\hline
6 & Probability of success (Predicted survival time) & Lowest & 
Sources acknowledge that mortality prediction and performance prognosis
are tools used and advocated for the optimal selection
of donors. &
\cite{ratul_survival_2022, islam_rifat_children_2023} \\
\hline
\end{tabularx}
\end{table*}



Expected Survival Time (in days) is estimated using a machine learning regression model trained and validated on a publicly available dataset of pediatric unrelated-donor bone marrow transplantations, containing 187 children and adolescents between the 2000 and 2008~\cite{Sikora2010}, derived from a published pediatric unrelated-donor cohort~\cite{Kalwak2010}. The variables of this dataset are described in table \ref{tab:all_attributes}. One of the predictive features used in this regression model is the Expected Probability of Survival, which represents a prediction of post-transplant survival within a follow-up window of three to eight years after transplantation. This survival probability is predicted using a separate regression machine learning model.
At the end of the pipeline, the system outputs an ordered ranking of donor–recipient pairs together with the estimated expected survival time. These results are presented to the clinician as decision support, enabling informed, transparent, and reproducible donor selection.

\subsection{Data Cleaning}


\begin{table}[htbp]
\caption{Clinical, Matching, Transplantation, and Outcome Attributes}
\label{tab:all_attributes}
\centering
\begin{tabularx}{\columnwidth}{l X}
\hline
\textbf{Attribute} & \textbf{Description} \\\\
\hline
\\
\multicolumn{2}{l}{\textbf{Donor-specific attributes}} \\
donor\_age & Refers to the donor age at donation \\
donor\_age\_below\_35 & 35 years cutoff age that has significantly lower risk of grade II to IV acute GVHD and lower likelihood of non-relapse mortality with miss-matched recipients \cite{Nagler2024} \\
donor\_ABO & The blood type of the donor \\
donor\_CMV & Presence of cytomegalovirus infection. A virus that is harmless and asymptomatic to most people but can be life-threatening for people with compromised immune systems \\\\
\hline
\\
\multicolumn{2}{l}{\textbf{Recipient-specific attributes}} \\
recipient\_age & The donor age at transplant \\
recipient\_age\_below\_10 & 10 years cutoff \\
recipient\_age\_int & Stores an age bin text \\
recipient\_gender & The gender of the recipient \\
recipient\_body\_mass & The body mass of the recipient \\
recipient\_ABO & The blood type of the recipient \\
recipient\_rh & The rh of the recipient's blood \\
recipient\_CMV & Presence of cytomegalovirus infection \\
disease & Type of disease \\
disease\_group & Malignant disease or not \\
risk\_group & The explicit meaning is still to be discovered, but it's assumed to be a value based in disease and disease status to categorize patients into 2 risk groups with significantly different overall survival and progression-free survival on the basis of primarily differences in the relapse risk \cite{Armand2012} \\\\
\hline
\\
\multicolumn{2}{l}{\textbf{Matching-related attributes}} \\
gender\_match & Checks if female donor to male recipient or any other case \\
ABO\_match & If blood types are compatible \\
CMV\_status & Level of serological compatibility \\
HLA\_match & Allele level donor-recipient matching \\
HLA\_mismatch & If HLA match if superior 8 alleles \\
antigen & Difference of antigens between donor and recipient \\
allel & Difference of alleles between donor and recipient \\
HLA\_group\_1 & Description of donor-recipient matching/mismatching \\\\
\hline
\\
\multicolumn{2}{l}{\textbf{Transplantation-related attributes}} \\
stem\_cell\_source & Where the stem cells were obtained \\
tx\_post\_relapse & If it is the second transplant done (after relapse) \\
CD34\_x1e6\_per\_kg & The CD34$^{+}$ cell dose ($10^{6}$) per kg of recipient body weight \\
CD3\_x1e8\_per\_kg & The CD3$^{+}$ cell dose ($10^{8}$) per kg of recipient body weight \\
CD3\_to\_CD34\_ratio & The CD3$^{+}$ to CD34$^{+}$ ratio \\\\
\hline
\\
\multicolumn{2}{l}{\textbf{Survivability attributes}} \\
relapse & If the disease has recurred \\
survival\_time & Time in days the recipient survived from transplant to death (if dead); time in days the recipient is alive from transplantation to time of data collection (if alive) \\
survival\_status & If the recipient is dead or alive \\\\
\hline

\end{tabularx}
\end{table}

Several data quality issues were identified and addressed prior to model development. Missing values, placeholder symbols (e.g., ``?''), and anomalous entries were detected in variables such as donor\_CMV, CD3\_x1e8\_per\_kg, CD3\_to\_CD34\_ratio, ANC\_recovery, PLT\_recovery, time\_to\_acute\_GvHD\_III\_IV, extensive\_chronic\_GvHD, HLA\_match with caution. Analize the usefulness in outcome of some abstracted attributes like donor\_age\_belo\_35, recipient\_age\_below\_10, HLA\_mismatch. Encode various categorical values with onehot-encoding since there is a small range of categorical values. Creation of synthetic data attributes for tissue type and donor gender to make possible HLA and gender match calculation during multi-criteria decision making. Check and handle bias and minority classes in outcome features using stratification and oversampling. Handle highly correlated features by removing the less performing ones, simplifying the generalization with few data rows.

The dataset provides a lot of data to better understand the match and transplant variables influence in survivability. It will be useful for finding correlations and importance of each attribute during the allocation fase, but will need to be simplified in order to help the model predict and generalize better with the small sample size.

\subsection{Data Exploration}

One of the first steps in data exploration is to analyze if the target data is biased in any way. This can be done by checking the proportions of the \texttt{survival\_status} attribute. In this dataset, 54\% of people are alive, while 45\% are deceased, which means that the target is slightly unbalanced. Looking at the data, a possible important factor is the correlation between the ages of the recipient and the donor. Fig.\ref{fig:age_correlation} shows the correlation between the pairs:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/age_correlation.png}
    \caption{Age correlation of donor-recipient pairs}
    \label{fig:age_correlation}
\end{figure}

This graph shows that a lot of transplants are made with donor-recipient pairs where the donor is between the ages of 25--45 years old and the recipient between 5--15 years old. Another thing to explore could be the correlation between the age gap of the pair. However, analysis suggests that the donor-recipient age gap doesn't have a strong enough correlation with either survival time or survival status of the recipient. Another attribute worth exploring is how blood type compatibility affects survival. Exploratory analysis showed that, while in case there is a blood type match the rate of survival is very close to 50\%, in case of a blood type mismatch the rate of survival drops to close to 40\%, showing a possible small correlation between these attributes. Gender matching can also be an important factor to consider in compatibility; however, analysis shows that gender matching alone isn't sufficient to form a correlation.

\subsection{Feature Engineering}
Several features were added, deleted or altered in some way to better help the model predict the target variable. Some of the new engineered features include the addition of the \texttt{age\_gap} feature, which is calculated using the absolute difference between the recipient-donor pair; the binning of the \texttt{donor\_age} and \texttt{recipient\_age} features; and the \texttt{total\_mismatches} feature, which is calculated using the total number of mismatched allels and antigens between donor and recipient.

\subsection{Oversampling}
\subsubsection{SMOTE}
Regarding oversampling, two types were tested in this project. One of them is Synthetic Minority Over-sampling TEchnique (SMOTE) \cite{Chawla_2002}. SMOTE is used in unbalanced datasets to create more samples of the minority class using a k-nearest neighbours algorithm. This way, new samples are not equal to existing ones, but slightly different variations of existing minority class samples. In this case, slight variations of SMOTE were used such as SMOTE-NC (for nominal and countinous data) and Adaptive Synthetic Sampling (ADASYN).

\subsubsection{GANs}
Generative Adversarial Networks (GANs) utilize neural networks to learn and generate synthetic data, which makes them a more robust and powerful alternative to other techniques \cite{gans}.


\section{Experiments}
Approximately 150 experiments with different models and model configurations were made. To facilitate this, MLflow and Hydra were used for tracking and result reproducibility across model configurations.\cite{Zaharia2018MLflow} \cite{Yadan2019Hydra}


\subsection{Survival Classification}
For survival classification, several model families were tested. All models were tested in three different configurations: default parameters, tuned and tuned with SMOTE. Every tuned model was also tested with different feature engineering steps. Additionally, the best models were also tested using GANs with different combinations of epochs and synthetic sample sizes. Different number of training cross validation folds and train-test splits were also considered. Finally, different methods of hyperparameter optimization (Grid Search, Random Search and Optuna) were explored and compared. \cite{optuna_2019} 

As metrics, the F1-score on the test set was the main metric used for comparison but several other metrics were also used such as the accuracy, precision, recall and cross-validation F1-score of the train set.

Linear classification models, including the Logistic Regression were evaluated as a baseline. Tree ensemble models like the Random Forest (RF) and the Extra Trees (ET), and gradient boosting models like the XGBoost (XGB) and the LightGBM (LGBM) were also tested. Alternating Decision Trees (ADTrees) and Support Vector Machines (SVMs) were also considered.

\subsection{Survival Time Regression}

To predict survival time tests were performed on largely the same model families as the step above. For the linear models family, ridge regression and elastic-net were tested. All models were tested with and without the addition of the classification variable predicted by the best classification model, tuned and untuned. Additionally, the classification variable was tested being predicted as a binary variable or as a probability. Metrics such as the Root Mean Squared Error (RMSE), Mean Average Error (MAE) and the coefficient of determination ($R^2$), were saved for both train and test sets and used to compare the regression models. A baseline RMSE value was also set (RMSE = 913), which would represent a simple model in which every patient is given the average survival time.

\section{Results and Discussion}
The section evaluates the performance of different ML models for the prediction of both a classification target and a regression target and analyzes the impact of feature engineering, hyperparameter tuning and data augmentation techniques.

\subsection{Survival Classification}

Out of all the experiments, the tuned XGBoost with GAN-augmented data obtained the best result, with an F1-score of 0.65 on the test set, a precision of 0.57, a recall of 0.61 and an accuracy of 0.62. However, the model that, on average, achieved the best results was the Logistic Regression, having the base untuned model ranked as the best model not utilizing GANs.

Bayesian hyperparameter tuning using Optuna did not consistently outperform grid search or random search in test performance. However, it reached very similar results significantly faster and sometimes even outperforming other methods.

Further, it was observed that SMOTE oversampling for the minority class did not significantly help in predicting the patient survival, having the models with SMOTE achieved an F1-score on the test set of 0.45, and 0.44 without, with precision and recall being very similar. On the other hand, GAN oversampling improved significantly the results, reaching an average F1-score on the test set of 0.50, against 0.44 without. 

On average, the tuned models without oversampling had better overall performance than the untuned models, however, it is important to note that some untuned models performed significantly better than their tuned conter-parts, even when confining their grid search, as can be seen in Fig.\ref{fig:tune_vs_untune}. It was observed that simpler models like Logistic Regression and SVM often had better results than more complex models. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/tune_vs_untune.png}
    \caption{Performance of tuned (no oversampling) vs. untuned models}
    \label{fig:tune_vs_untune}
\end{figure}

This is possibly due to the small dataset size which causes many of the more complex models, even on default parameters, to overfit the training data, causing poor performance on the test set. Due to this, simpler models are often able to create a better generalization.

GAN-augmentation had a significant impact in model performance for tree models, while negatively affecting linear models. Gradient boosting models also were positively affected. Several GAN configuration were attempted and it was observed that the one that produced the best results overall was a sample size of 10 synthetic entries for class 0, 20 synthetic entries for class 1 and the the number of epochs set to 300.

This observation is consistent with the previous hypothesis. With more data, tree models don't overfit the training set as much, and get better results. However, it is important to note that data quality also plays a big role, because simply generating more data using GANs doesn't always yield better results. With little data, GANs can also overfit the train set and generate synthetic data that is too close to the real set which only adds noise. On the other hand, reducing the number of epochs too much can also cause them to not learn enough about the data. 

Another observation made was when comparing the cross-validation F1-Score on the train set with the test set. As can be seen in the sample of parallel coordinates plot in Fig.\ref{fig:train_vs_test}, 12 random experiments were chosen, and compared. It was observed that experiments that scored a high F1-score on cross-validation with the train set performed consistently worse on the test set than those that scored a lower value.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\columnwidth]{figures/train_vs_test.png}
    \caption{Comparison between train and test set performance across experiments}
    \label{fig:train_vs_test}
\end{figure}
\FloatBarrier

Some additional experiments were made on the best-performing models comparing different values for number of cross validation folds and train/test split ratios. It was observed that training the model with 5 folds and a test set size of 15\% of the total dataset yielded better results.

\subsection{Survival Time Regression}

As can be seen in Fig. \ref{fig:use_clf}, using classification as a feature to predict regression considerably improved model performance.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/classification_influence.png}
    \caption{Use of classification as feature for regression}
    \label{fig:use_clf}
\end{figure}

Using the red line to indicate the baseline level, the RMSE of the models not using classification were all observed to be above that level. However, by adding the survival status as a feature the RMSE was cut almost in half.
Moreover, predicting the survivability as a probability instead of a binary feature produced mixed results across models and configurations, with tree and linear models performing particularly bad, however, gradient boosting models like LightGBM and XGBoost had better results than the same model with a binary classification feature.

As with classification, different numbers of cross-validation folds were tested and 5 folds was observed to produce the best results.

In the end, the regression model that preformed best was a tuned Extra Trees Regressor model using survival status as a probability.

\subsection{SHAP Analysis}
To improve model explainability, a general SHapley Additive exPlanation (SHAP) analysis was performed and plotted for the best-performing classification and regression model~\cite{Lundberg2017SHAP}.
Each dot represents a single data point of the respective feature.
The x-axis represents the SHAP value, which is a measure of the impact of each feature on the model's prediction (a negative shap means the model is more likely to predict the patient's survival, while a positive value means the opposite). The color of each dot represents the value of the feature for that data point.
A red feature represents a high value and a blue feature represents a low value.
For example, in the feature \texttt{age\_gap}, a red dot represents a high age gap and a blue dot represents a low age gap.
The ordering along the y-axis does not affect interpretability and is used only to improve visualization.
It should be noted that SHAP analysis was performed strictly post hoc and was not used to guide feature engineering decisions.
Fig.\ref{fig:clf_shap} shows a beeswarm plot of the most important features for the classification model.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/clf_shap.png}
    \caption{SHAP Analysis for classification model}
    \label{fig:clf_shap}
\end{figure}

Fig.\ref{fig:reg_shap} shows a beeswarm plot of the top 3 features for the regression model.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/reg_shap.png}
    \caption{SHAP Analysis for regression model}
    \label{fig:reg_shap}
\end{figure}

Some features are very strong and clean indicators of the model's prediction. For example, the feature \texttt{risk\_group\_low} being positive is a big indicator that the model will predict the patients survival. To add to this, we can observe that most binary features form separated clusters, indicating a consistent directional impact on the model's prediction depending on feature value. However, that's not always the case, as we can see in the feature \texttt{ABO\_match\_mismatched} or \texttt{donor\_CMV\_present}. We can also observe that the engineered features \texttt{age\_gap} and \texttt{recipient\_age\_bin} had a strong impact on the model's prediction. 

Furthermore, we can clearly see the importance of predicting the survival status of the patient as a feature for the regression model.

\section{Conclusion}

\section*{Acknowledgment}
The authors thank the lecturers of the Master in Artificial Intelligence Engineering (MEIA) at the Porto School of Engineering (ISEP) for the guidance and support provided throughout the development of this project.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}